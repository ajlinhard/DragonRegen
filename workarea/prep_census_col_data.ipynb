{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "s_raw_root_path = r'F:\\DataSamples\\DataSets'\n",
    "s_spark_file_server_root = r'F:\\Spark_Data_Test'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Prep Census Data') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", s_spark_file_server_root) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+--------+--------+------------+--------+--------+------+-------+---------+-----------+\n",
      "|name           |rank|count   |prop100k|cum_prop100k|pctwhite|pctblack|pctapi|pctaian|pct2prace|pcthispanic|\n",
      "+---------------+----+--------+--------+------------+--------+--------+------+-------+---------+-----------+\n",
      "|ALL OTHER NAMES|0   |29312001|9936.97 |9936.97     |66.65   |8.53    |7.97  |0.86   |2.32     |13.67      |\n",
      "|SMITH          |1   |2442977 |828.19  |828.19      |70.9    |23.11   |0.5   |0.89   |2.19     |2.4        |\n",
      "|JOHNSON        |2   |1932812 |655.24  |1483.42     |58.97   |34.63   |0.54  |0.94   |2.56     |2.36       |\n",
      "|WILLIAMS       |3   |1625252 |550.97  |2034.39     |45.75   |47.68   |0.46  |0.82   |2.81     |2.49       |\n",
      "|BROWN          |4   |1437026 |487.16  |2521.56     |57.95   |35.6    |0.51  |0.87   |2.55     |2.52       |\n",
      "|JONES          |5   |1425470 |483.24  |3004.8      |55.19   |38.48   |0.44  |1      |2.61     |2.29       |\n",
      "|GARCIA         |6   |1166120 |395.32  |3400.12     |5.38    |0.45    |1.41  |0.47   |0.26     |92.03      |\n",
      "|MILLER         |7   |1161437 |393.74  |3793.86     |84.11   |10.76   |0.54  |0.66   |1.77     |2.17       |\n",
      "|DAVIS          |8   |1116357 |378.45  |4172.31     |62.2    |31.6    |0.49  |0.82   |2.45     |2.44       |\n",
      "|RODRIGUEZ      |9   |1094924 |371.19  |4543.5      |4.75    |0.54    |0.57  |0.18   |0.18     |93.77      |\n",
      "+---------------+----+--------+--------+------------+--------+--------+------+-------+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Files\n",
    "df_census_surname = spark.read \\\n",
    "    .options(header='true', delimiter=',', inferSchema=True) \\\n",
    "    .csv(os.path.join(s_raw_root_path, 'Census_Surnames\\\\Names_2010Census.csv'))\n",
    "\n",
    "df_census_surname.orderBy('rank', ascending=True).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a upper bound for cumulative distribution function\n",
    "df_census = df_census_surname.filter(col('name') != 'ALL OTHER NAMES').withColumn('unqiue_rank', row_number().over(Window.orderBy('cum_prop100k')))\n",
    "df_join = df_census.alias('high').join(df_census.alias('low'), on= col('low.unqiue_rank') == col('high.unqiue_rank') - 1, how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-----------+----------------+-----------------+\n",
      "|     name|     name|     name|unqiue_rank|cum_prop100k_low|cum_prop100k_high|\n",
      "+---------+---------+---------+-----------+----------------+-----------------+\n",
      "|    SMITH|     NULL|    SMITH|          1|             0.0|           828.19|\n",
      "|  JOHNSON|    SMITH|  JOHNSON|          2|          828.19|          1483.42|\n",
      "| WILLIAMS|  JOHNSON| WILLIAMS|          3|         1483.42|          2034.39|\n",
      "|    BROWN| WILLIAMS|    BROWN|          4|         2034.39|          2521.56|\n",
      "|    JONES|    BROWN|    JONES|          5|         2521.56|           3004.8|\n",
      "|   GARCIA|    JONES|   GARCIA|          6|          3004.8|          3400.12|\n",
      "|   MILLER|   GARCIA|   MILLER|          7|         3400.12|          3793.86|\n",
      "|    DAVIS|   MILLER|    DAVIS|          8|         3793.86|          4172.31|\n",
      "|RODRIGUEZ|    DAVIS|RODRIGUEZ|          9|         4172.31|           4543.5|\n",
      "| MARTINEZ|RODRIGUEZ| MARTINEZ|         10|          4543.5|           4902.9|\n",
      "+---------+---------+---------+-----------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----------+-----------+-----------+----------------+-----------------+\n",
      "|       name|       name|       name|unqiue_rank|cum_prop100k_low|cum_prop100k_high|\n",
      "+-----------+-----------+-----------+-----------+----------------+-----------------+\n",
      "|       NULL|    DORIOTT|      OTHER|     162254|        90063.03|         100000.0|\n",
      "|    DORIOTT|     DONLEA|    DORIOTT|     162253|        90062.99|         90063.03|\n",
      "|     DONLEA|      DOKAS|     DONLEA|     162252|        90062.96|         90062.99|\n",
      "|      DOKAS|  DIETZMANN|      DOKAS|     162251|        90062.93|         90062.96|\n",
      "|  DIETZMANN|     DOBBEN|  DIETZMANN|     162250|        90062.89|         90062.93|\n",
      "|     DOBBEN|     DOBRON|     DOBBEN|     162249|        90062.86|         90062.89|\n",
      "|     DOBRON| DITERLIZZI|     DOBRON|     162248|        90062.83|         90062.86|\n",
      "| DITERLIZZI| DIFILLIPPO| DITERLIZZI|     162247|        90062.79|         90062.83|\n",
      "| DIFILLIPPO|DONNERMEYER| DIFILLIPPO|     162246|        90062.76|         90062.79|\n",
      "|DONNERMEYER|   DOCHNIAK|DONNERMEYER|     162245|        90062.72|         90062.76|\n",
      "+-----------+-----------+-----------+-----------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = df_join.select(\n",
    "    col('high.name'),col('low.name'),\n",
    "    coalesce(col('high.name'),lit('OTHER')).alias('name'),\n",
    "    coalesce(col('high.unqiue_rank'),col('low.unqiue_rank')+1).alias('unqiue_rank'),\n",
    "    ifnull(col('low.cum_prop100k'), lit(0)).alias('cum_prop100k_low'),\n",
    "    ifnull(col('high.cum_prop100k'), lit(100_000)).alias('cum_prop100k_high'),\n",
    ")\n",
    "\n",
    "df_join.orderBy(col('cum_prop100k_low'), ascending=True\n",
    ").show(10)\n",
    "df_join.orderBy(col('cum_prop100k_low'), ascending=False\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_ACCEPT_OBJECT_IN_TYPE] `StructType` can not accept object `53430` in type `int`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDataCreator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDataGenerators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPyData\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyData\n\u001b[0;32m      3\u001b[0m li_name_lkp \u001b[38;5;241m=\u001b[39m PyData\u001b[38;5;241m.\u001b[39mrandom_ints(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100_000\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m df_new_names \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mli_name_lkp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStructField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname_int\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIntegerType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m df_new_name_w_str \u001b[38;5;241m=\u001b[39m df_new_names\u001b[38;5;241m.\u001b[39mjoin(df_join, on\u001b[38;5;241m=\u001b[39m (df_new_names\u001b[38;5;241m.\u001b[39mname_int \u001b[38;5;241m>\u001b[39m df_join\u001b[38;5;241m.\u001b[39mcum_prop100k_low \u001b[38;5;129;01mand\u001b[39;00m df_new_names\u001b[38;5;241m.\u001b[39mname_int \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m df_join\u001b[38;5;241m.\u001b[39mcum_prop100k_high), how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\sql\\session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1442\u001b[0m     )\n\u001b[1;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\sql\\session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\sql\\session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[1;32m-> 1459\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\sql\\types.py:2201\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[1;32m-> 2201\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\sql\\types.py:2180\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2178\u001b[0m         verifier(d\u001b[38;5;241m.\u001b[39mget(f))\n\u001b[0;32m   2179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   2181\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_ACCEPT_OBJECT_IN_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2182\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   2183\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStructType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2184\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(obj),\n\u001b[0;32m   2185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m   2186\u001b[0m         },\n\u001b[0;32m   2187\u001b[0m     )\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `StructType` can not accept object `53430` in type `int`."
     ]
    }
   ],
   "source": [
    "from src.DataCreator.DataGenerators.PyData import PyData\n",
    "\n",
    "li_name_lkp = PyData.random_ints(1000, 1, 100_000)\n",
    "df_new_names = spark.createDataFrame(li_name_lkp, StructType([StructField('name_int', IntegerType(), False)]))\n",
    "\n",
    "df_new_name_w_str = df_new_names.join(df_join, on= (df_new_names.name_int > df_join.cum_prop100k_low and df_new_names.name_int <= df_join.cum_prop100k_high), how='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
