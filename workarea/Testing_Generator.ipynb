{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalej\\Documents\\_Coding\\DragonRegen\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "print(project_root)\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from DataCreator.DataSets.DataSetGenStandard import DataSetGenStandard\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test Data Set Generation\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "from DataCreator.DataGenerators.FirstNameData import FirstNameData\n",
    "from DataCreator.DataGenerators.LastNameData import LastNameData\n",
    "# Load Frst and Last Name data sets\n",
    "first_name_data_set = FirstNameData(spark)\n",
    "last_name_data_set = LastNameData(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Base Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Column Name:id\n",
      "Data Type: IntegerType()\n",
      "Checking subclass: StringBasic and requirements: None\n",
      "Checking subclass: ColBasic and requirements: <class 'DataCreator.ColGenerators.ColBasic.ColBasic'>\n",
      "==> Column Name:first_name\n",
      "Checking subclass: StringBasic and requirements: <class 'DataCreator.ColGenerators.StringFirstName.StringFirstName'>\n",
      "==> Column Name:last_name\n",
      "Checking subclass: StringBasic and requirements: <class 'DataCreator.ColGenerators.StringLastName.StringLastName'>\n",
      "==> Column Name:age\n",
      "Data Type: IntegerType()\n",
      "Checking subclass: StringBasic and requirements: None\n",
      "Checking subclass: ColBasic and requirements: <class 'DataCreator.ColGenerators.ColBasic.ColBasic'>\n",
      "==> Column Name:city\n",
      "Checking subclass: StringBasic and requirements: <class 'DataCreator.ColGenerators.StringBasic.StringBasic'>\n",
      "Generating Rows: 100, with Columns: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"f:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"f:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"f:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle '_thread.RLock' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle '_thread.RLock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\serializers.py:459\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[0;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m data_gen \u001b[38;5;241m=\u001b[39m DataSetGenStandard(spark, testSchema, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Generate the DataFrame\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(df))\n\u001b[0;32m     17\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Documents\\_Coding\\DragonRegen\\src\\DataCreator\\DataSets\\DataSetGenStandard.py:47\u001b[0m, in \u001b[0;36mDataSetGenStandard.generate_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(ColGenerator\u001b[38;5;241m.\u001b[39mcreate(field\u001b[38;5;241m.\u001b[39mname, field\u001b[38;5;241m.\u001b[39mdataType, field\u001b[38;5;241m.\u001b[39mnullable, field\u001b[38;5;241m.\u001b[39mmetadata)\u001b[38;5;241m.\u001b[39mgenerate_column(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_row_count))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating Rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_row_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, with Columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\sql\\session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1442\u001b[0m     )\n\u001b[1;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\sql\\session.py:1116\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m internal_data \u001b[38;5;241m=\u001b[39m [struct\u001b[38;5;241m.\u001b[39mtoInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tupled_data]\n\u001b[1;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal_data\u001b[49m\u001b[43m)\u001b[49m, struct\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\context.py:824\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonParallelizeServer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), numSlices)\n\u001b[1;32m--> 824\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_to_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateRDDServer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\context.py:867\u001b[0m, in \u001b[0;36mSparkContext._serialize_to_jvm\u001b[1;34m(self, data, serializer, reader_func, server_func)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 867\u001b[0m         \u001b[43mserializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m         tempFile\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\serializers.py:225\u001b[0m, in \u001b[0;36mBatchedSerializer.dump_stream\u001b[1;34m(self, iterator, stream)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator, stream):\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\serializers.py:147\u001b[0m, in \u001b[0;36mFramedSerializer.dump_stream\u001b[1;34m(self, iterator, stream)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator, stream):\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m--> 147\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_with_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\serializers.py:157\u001b[0m, in \u001b[0;36mFramedSerializer._write_with_length\u001b[1;34m(self, obj, stream)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write_with_length\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, stream):\n\u001b[1;32m--> 157\u001b[0m     serialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m serialized \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialized value should not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\Python\\envs\\PySpark\\lib\\site-packages\\pyspark\\serializers.py:469\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    467\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[0;32m    468\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not serialize object: TypeError: cannot pickle '_thread.RLock' object"
     ]
    }
   ],
   "source": [
    "# Create a DataSetGenStandard object\n",
    "testSchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True, {\"ColType\": \"FirstName\"}),\n",
    "    StructField(\"last_name\", StringType(), True, {\"ColType\": \"LastName\"}),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    # StructField(\"gender\", StringType(), True, metadata={'column_value': ['M', 'F']}),\n",
    "    # StructField(\"gender\", StringType(), True, {'column_value': ['M', 'F'], 'value_ratio': [0.5, 0.5]}),\n",
    "    # StructField(\"transaction_date\", DateType(), True),\n",
    "])\n",
    "\n",
    "data_gen = DataSetGenStandard(spark, testSchema, 100)\n",
    "# Generate the DataFrame\n",
    "# TODO: So things are a list some are DataFrames (First and Last Name)\n",
    "df = data_gen.generate_data()\n",
    "print(type(df))\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test All major data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Rows: 1755, with Columns: 8\n",
      "The DataFrame count is: 1755\n",
      "+---+--------------------+---+------------------+---------+----------------+-------------------+-------------------+\n",
      "|id |name                |age|city              |salary   |transaction_date|insert_datetime    |update_datetime    |\n",
      "+---+--------------------+---+------------------+---------+----------------+-------------------+-------------------+\n",
      "|64 |ina4fwtlApEGhdXa    |75 |eq2wQrFMoGX8NZ6HTv|64.64829 |2023-11-21      |2022-08-03 19:45:21|2023-03-16 23:52:15|\n",
      "|30 |IHz                 |30 |11Vzj             |87.80684 |2021-02-24      |2022-06-04 19:33:58|2023-09-03 01:57:53|\n",
      "|46 |PKYz                |41 |RJUWm             |80.58904 |2022-01-18      |2023-07-04 22:06:36|2023-01-15 12:45:09|\n",
      "|51 |0briSy9UtY          |36 |XGapB3cuS9B       |24.221617|2021-03-25      |2022-03-13 10:41:58|2021-09-13 05:51:24|\n",
      "|10 |JmOaEQKTizvaxNy     |82 |I7Df9Jx73KhuxQS   |77.777504|2020-08-14      |2020-01-07 16:41:48|2020-01-30 07:54:23|\n",
      "|72 |eE4fs4PDdlr         |71 |XSo4kBN           |54.169518|2022-05-31      |2021-01-24 18:38:41|2023-03-02 13:35:24|\n",
      "|69 |M7i                 |28 |DutcSFrEH21FmHoB  |66.37854 |2022-07-21      |2022-03-27 09:41:47|2020-01-10 14:55:49|\n",
      "|18 |6IXlGANBfT4NVWzKTHRz|73 |8I9acB1rfnbI      |47.144905|2023-04-02      |2022-07-17 11:46:30|2023-05-06 13:06:06|\n",
      "|55 |7Qyorss6HtCduUM     |49 |rG4viC            |92.70449 |2023-09-19      |2023-03-10 19:19:30|2022-11-19 05:05:47|\n",
      "|55 |pqbW4Kdil           |58 |bXuRvlbf          |62.022358|2021-02-18      |2022-10-17 21:25:44|2021-07-01 08:51:35|\n",
      "+---+--------------------+---+------------------+---------+----------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataSetGenStandard object\n",
    "testSchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"salary\", FloatType(), True),\n",
    "    StructField(\"transaction_date\", DateType(), True),\n",
    "    StructField(\"insert_datetime\", TimestampNTZType(), True),\n",
    "    StructField(\"update_datetime\", TimestampNTZType(), True),\n",
    "])\n",
    "\n",
    "data_gen_all_types = DataSetGenStandard(spark, testSchema, 1755)\n",
    "# Generate the DataFrame\n",
    "df_all_types = data_gen_all_types.generate_data()\n",
    "print('The DataFrame count is:', end=' ')\n",
    "print(df_all_types.count())\n",
    "df_all_types.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'int'), ('name', 'string'), ('age', 'int'), ('city', 'string'), ('salary', 'float'), ('transaction_date', 'date'), ('insert_datetime', 'timestamp_ntz'), ('update_datetime', 'timestamp_ntz')]\n",
      "('id', 'int')\n",
      "<class 'list'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(df_all_types.dtypes)\n",
    "print(df_all_types.dtypes[0])\n",
    "print(type(df_all_types.dtypes))\n",
    "print(type(df_all_types.dtypes[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Replication Class Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'DataCreator.DataSets.DataSetGenStandard.DataSetGenStandard'>\n",
      "<class 'pyspark.sql.types.StructType'>\n",
      "StructType([StructField('id', IntegerType(), True), StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('city', StringType(), True), StructField('salary', FloatType(), True), StructField('transaction_date', DateType(), True), StructField('insert_datetime', TimestampNTZType(), True), StructField('update_datetime', TimestampNTZType(), True)])\n"
     ]
    }
   ],
   "source": [
    "ds_replicate = DataSetGenStandard.replicate_data(spark, df_all_types)\n",
    "print(type(ds_replicate))\n",
    "print(type(ds_replicate.schema))\n",
    "print(ds_replicate.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Rows: 1000, with Columns: 8\n",
      "+---+------------------+---+------------------+---------+----------------+-------------------+-------------------+\n",
      "|id |name              |age|city              |salary   |transaction_date|insert_datetime    |update_datetime    |\n",
      "+---+------------------+---+------------------+---------+----------------+-------------------+-------------------+\n",
      "|33 |p7Bt11olw0        |96 |uApwxdUGersC1m    |61.674034|2020-12-28      |2020-08-01 02:56:09|2021-04-22 21:49:30|\n",
      "|70 |oBeo7fwuu         |6  |TXE17             |26.416477|2022-06-09      |2021-05-16 21:19:23|2022-11-02 02:24:04|\n",
      "|17 |FzXhamsqv8RXS     |59 |bVdgKpgxTMfivJWTuT|62.96358 |2023-05-23      |2023-02-05 15:46:37|2023-06-21 14:46:52|\n",
      "|92 |zzIJyXDDF9gGWBgicx|16 |EDdMMnR           |33.541084|2021-04-01      |2023-08-30 04:12:37|2023-02-24 13:08:22|\n",
      "|20 |MoZYfxEp2JRUvwEAB |91 |0xJeIul           |31.65818 |2023-04-21      |2021-01-16 17:52:41|2021-09-14 23:47:12|\n",
      "|65 |PYz6tLNJHsimvoqpg |62 |eWc               |67.09343 |2022-07-24      |2021-01-30 04:02:36|2022-09-17 18:16:35|\n",
      "|30 |gAGc              |74 |sPqvm             |81.74553 |2023-01-27      |2022-08-23 06:54:46|2020-12-09 11:19:01|\n",
      "|84 |UoouAHE9NNEffeD   |29 |HHhvc             |16.42443 |2021-06-20      |2020-09-11 08:00:38|2022-02-20 16:38:49|\n",
      "|4  |Nc7T              |79 |Zm7K2PS           |86.663795|2023-07-21      |2023-08-15 23:09:30|2023-07-10 12:12:53|\n",
      "|56 |QR                |42 |NGF6pdSjpyyyB     |73.73573 |2021-05-10      |2023-10-12 23:23:01|2021-04-14 13:51:28|\n",
      "+---+------------------+---+------------------+---------+----------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rep = ds_replicate.generate_data()\n",
    "df_rep.show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
